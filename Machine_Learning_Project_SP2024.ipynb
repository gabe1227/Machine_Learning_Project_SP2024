{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "83b8a794-093e-4925-9488-c54e8b5915a7",
   "metadata": {},
   "source": [
    "# CS345 Spring 2024 Project - Gabriel Schonacher\n",
    "\n",
    "## March Madness Dataset - Predicition\n",
    "\n",
    "We are importing the March Madness Dataset and creating a prediction with the use of SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f98bf9c-9df7-4781-934e-b7fed0815aa3",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with '/bin/python3.11' requires the ipykernel package.\n",
      "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: '/bin/python3.11 -m pip install ipykernel -U --user --force-reinstall'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import cross_validate\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import cross_val_predict\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c9a4004-d6d6-408f-95dc-15e4580c91aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "Eff_March_Madness = \"INT _ KenPom _ Efficiency.csv\"\n",
    "\n",
    "Eff_March_Madness = pd.read_csv(Eff_March_Madness)\n",
    "\n",
    "Eff_March_Madness_array = np.array(Eff_March_Madness)\n",
    "\n",
    "np.random.shuffle(Eff_March_Madness_array)\n",
    "\n",
    "print(np.shape(Eff_March_Madness_array))\n",
    "\n",
    "X = np.array(Eff_March_Madness_array[:, :-1])\n",
    "y = np.array(Eff_March_Madness_array[:, -1])\n",
    "\n",
    "X.shape, y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a219a6c-ffc4-4592-a7f4-d911d6f8d394",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with '/bin/python3.11' requires the ipykernel package.\n",
      "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: '/bin/python3.11 -m pip install ipykernel -U --user --force-reinstall'"
     ]
    }
   ],
   "source": [
    "y = np.where(y == 1, 1, -1)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.3, random_state=5)\n",
    "\n",
    "p = perceptron(iterations = 250, learning_rate = 0.1)\n",
    "p.fit(X_train, y_train)\n",
    "y_pred = p.predict(X_test)\n",
    "\n",
    "p_hb_mean = np.mean(y_test == y_pred)\n",
    "p_hb_std = np.std(y_test == y_pred)\n",
    "\n",
    "print(\"Perceptron Habber-man Average: \", p_hb_mean)\n",
    "print(\"Perceptron Habber-man Standard Deviation: \", p_hb_std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75e7bace-f0fe-4347-9f5e-0e0e9d68b29a",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with '/bin/python3.11' requires the ipykernel package.\n",
      "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: '/bin/python3.11 -m pip install ipykernel -U --user --force-reinstall'"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.3, random_state=5)\n",
    "\n",
    "svm_classifer = SVC(kernel=\"linear\")\n",
    "\n",
    "svm_classifer.fit(X_train, y_train)\n",
    "y_pred = svm_classifer.predict(X_test)\n",
    "\n",
    "p_hb_mean = np.mean(y_test == y_pred)\n",
    "p_hb_std = np.std(y_test == y_pred)\n",
    "\n",
    "print(\"SVM Habber-man Average: \", p_hb_mean)\n",
    "print(\"SVM Habber-man Standard Deviation: \", p_hb_std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e37efab3-405f-465d-b5a4-9e712b8e5888",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with '/bin/python3.11' requires the ipykernel package.\n",
      "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: '/bin/python3.11 -m pip install ipykernel -U --user --force-reinstall'"
     ]
    }
   ],
   "source": [
    "kf_classifer = KFold(n_splits=10,shuffle=True,random_state=5)\n",
    "\n",
    "hb_average = []\n",
    "\n",
    "for i, (train, test) in enumerate(kf_classifer.split(X)):\n",
    "    X_train, X_test = X[train], X[test]\n",
    "    y_train, y_test = y[train], y[test]\n",
    "    \n",
    "    p = perceptron(iterations=250, learning_rate=0.01)\n",
    "    p.fit(X_train, y_train)\n",
    "    y_pred = p.predict(X_test)\n",
    "    av = np.mean(y_test == y_pred)\n",
    "    hb_average.append(av)\n",
    "\n",
    "ten_hab_average = np.mean(hb_average)\n",
    "\n",
    "print(\"10 fold for Habberman Dataset with Perceptron\", ten_hab_average)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66f4399f-51b3-4d03-a8f5-2d44c9f68205",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with '/bin/python3.11' requires the ipykernel package.\n",
      "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: '/bin/python3.11 -m pip install ipykernel -U --user --force-reinstall'"
     ]
    }
   ],
   "source": [
    "kf_classifer = KFold(n_splits=10,shuffle=True,random_state=5)\n",
    "\n",
    "hb_average = []\n",
    "\n",
    "for i, (train, test) in enumerate(kf_classifer.split(X)):\n",
    "    X_train, X_test = X[train], X[test]\n",
    "    y_train, y_test = y[train], y[test]\n",
    "    \n",
    "    svm_classifer = SVC(kernel=\"linear\")\n",
    "    svm_classifer.fit(X_train, y_train)\n",
    "    y_pred = svm_classifer.predict(X_test)\n",
    "    av = np.mean(y_test == y_pred)\n",
    "    hb_average.append(av)\n",
    "\n",
    "ten_hab_average = np.mean(hb_average)\n",
    "\n",
    "print(\"10 fold for Habberman Dataset with SVC\", ten_hab_average)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0327f2ae-a6f7-4dd0-a1b8-d634b519db06",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with '/bin/python3.11' requires the ipykernel package.\n",
      "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: '/bin/python3.11 -m pip install ipykernel -U --user --force-reinstall'"
     ]
    }
   ],
   "source": [
    "data = load_breast_cancer()\n",
    "\n",
    "X = data.data\n",
    "y = data.target\n",
    "\n",
    "y = np.where(y == 1, 1, -1)\n",
    "\n",
    "X.shape, y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fdb8f7d-ff50-4ffe-880c-0402e14653ea",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with '/bin/python3.11' requires the ipykernel package.\n",
      "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: '/bin/python3.11 -m pip install ipykernel -U --user --force-reinstall'"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.4, \n",
    "                                                    random_state=0)\n",
    "\n",
    "svm_classifer = SVC(kernel=\"linear\")\n",
    "\n",
    "svm_classifer.fit(X_train, y_train)\n",
    "y_pred = svm_classifer.predict(X_test)\n",
    "\n",
    "breast_cancer_average = np.mean(y_pred == y_test)\n",
    "breast_cancer_standard_deviation = np.std(y_pred == y_test)\n",
    "\n",
    "print(\"SVM Breast Cancer Average: \", breast_cancer_average)\n",
    "print(\"SVM Breast Cacner Standard Deviation: \", breast_cancer_standard_deviation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78b331b4-a44b-498b-88ab-330f3c4fc385",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with '/bin/python3.11' requires the ipykernel package.\n",
      "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: '/bin/python3.11 -m pip install ipykernel -U --user --force-reinstall'"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.4, \n",
    "                                                    random_state=0)\n",
    "\n",
    "p = perceptron(iterations = 250, learning_rate = 0.1)\n",
    "p.fit(X_train, y_train)\n",
    "y_pred = p.predict(X_test)\n",
    "\n",
    "breast_cancer_average = np.mean(y_pred == y_test)\n",
    "breast_cancer_standard_deviation = np.std(y_pred == y_test)\n",
    "\n",
    "print(\"Perceptron Breast Cancer Average: \", breast_cancer_average)\n",
    "print(\"Perceptron Breast Cacner Standard Deviation: \", breast_cancer_standard_deviation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e72f0ec-2641-4280-96a0-d5938b07b8b4",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with '/bin/python3.11' requires the ipykernel package.\n",
      "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: '/bin/python3.11 -m pip install ipykernel -U --user --force-reinstall'"
     ]
    }
   ],
   "source": [
    "kf_classifer = KFold(n_splits=10,shuffle=True,random_state=5)\n",
    "\n",
    "breCan_average = []\n",
    "\n",
    "for i, (train, test) in enumerate(kf_classifer.split(X)):\n",
    "    X_train, X_test = X[train], X[test]\n",
    "    y_train, y_test = y[train], y[test]\n",
    "    \n",
    "    svm_classifer = SVC(kernel=\"linear\")\n",
    "    svm_classifer.fit(X_train, y_train)\n",
    "    y_pred = svm_classifer.predict(X_test)\n",
    "    av = np.mean(y_test == y_pred)\n",
    "    breCan_average.append(av)\n",
    "\n",
    "ten_breCan_average = np.mean(breCan_average)\n",
    "\n",
    "print(\"10 fold for Breast Cancer Dataset with SVC\", ten_breCan_average)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c07d9601-fde4-444e-b446-10c209173aa2",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with '/bin/python3.11' requires the ipykernel package.\n",
      "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: '/bin/python3.11 -m pip install ipykernel -U --user --force-reinstall'"
     ]
    }
   ],
   "source": [
    "kf_classifer = KFold(n_splits=10,shuffle=True,random_state=5)\n",
    "\n",
    "breCan_average = []\n",
    "\n",
    "for i, (train, test) in enumerate(kf_classifer.split(X)):\n",
    "    X_train, X_test = X[train], X[test]\n",
    "    y_train, y_test = y[train], y[test]\n",
    "    \n",
    "    p_classifer = perceptron(iterations=1000, learning_rate=0.02, seed=5)\n",
    "    p_classifer.fit(X_train, y_train)\n",
    "    y_pred = p_classifer.predict(X_test)\n",
    "    av = np.mean(y_test == y_pred)\n",
    "    breCan_average.append(av)\n",
    "\n",
    "ten_breCan_average = np.mean(breCan_average)\n",
    "\n",
    "print(\"10 fold for Breast Cancer Dataset with Preceptron\", ten_breCan_average)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6a4dcb4-ce3d-49a2-ae16-916da669f39f",
   "metadata": {},
   "source": [
    "*Discussion of the results here*.\n",
    "\n",
    "\n",
    "*The Habber-Man Average is lower than the Breast Cancer data set, and the Standard Deviation is higher in the higher in the Habber-man dataset than the Breast Cancer dataset making the Breast Cancer dataset having more consistency. While the Standard Deviation in the Habber-man data is higher, suggesting that it has higher variabilty than the Breast Cancer dataset. While the 10 Fold for Breast cancer test for the preceptron and the SVC is relativley higher than the perceptron. While for the Habberman data set in regards to the 10 Fold, it is higher of an accuracy for the SVC compared to the perceptron*."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c3ce3b7-61bd-45f9-a775-e89fd8e69cd4",
   "metadata": {},
   "source": [
    "## Part 2 - Learning Curve\n",
    "\n",
    "We looked briefly at the idea of learning curves in the notebook for cross-validation. It looked at how accuracy changed with respect to the number of training examples. In this part of the assignment, plot a learning curve for the perceptron algorithm for an increasing number of training examples. To plot this, \n",
    "1. First, create a held-out validation set against which you will be comparing all your trained models\n",
    "2. Now, from the remaining data set, create train sets of increasing sizes. To create more compact plots you can use a logarithmic scale like 10, 20, 40, 80, 160, etc..\n",
    "\n",
    "For the plot X-axis should be the training data size considered and the Y-axis should be the accuracy of the model (obtained by training on the dataset of that size) as measured on the held-out validation set.  \n",
    "\n",
    "After receiving the plot, make sure to discuss your observations about the plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef3256b0-c4b2-4b43-bea6-a2454d86ba95",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with '/bin/python3.11' requires the ipykernel package.\n",
      "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: '/bin/python3.11 -m pip install ipykernel -U --user --force-reinstall'"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_breast_cancer\n",
    "\n",
    "data = load_breast_cancer()\n",
    "\n",
    "X = data.data\n",
    "y = data.target\n",
    "\n",
    "y = np.where(y == 1, 1, -1)\n",
    "\n",
    "X.shape, y.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c246835e-57dd-4cb3-ba38-5c2558ac6dd8",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with '/bin/python3.11' requires the ipykernel package.\n",
      "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: '/bin/python3.11 -m pip install ipykernel -U --user --force-reinstall'"
     ]
    }
   ],
   "source": [
    "X_full_train, X_test, y_full_train, y_test = train_test_split(\n",
    "    X, y, test_size = len(y)-max(training_sizes), \n",
    "    stratify=y, random_state=1)\n",
    "\n",
    "print(f'We have selected {X_test.shape[0]} test images from {X.shape[0]} images total.')\n",
    "print(f'The remaining {X_full_train.shape[0]} images are available for training.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29f4b71d-f062-4407-839d-765191f9c2b1",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with '/bin/python3.11' requires the ipykernel package.\n",
      "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: '/bin/python3.11 -m pip install ipykernel -U --user --force-reinstall'"
     ]
    }
   ],
   "source": [
    "training_sizes = [10, 20, 40, 80, 160]\n",
    "\n",
    "accuracy = []\n",
    "\n",
    "for training_size in training_sizes:\n",
    "    X_train,_ , y_train,_ = train_test_split(\n",
    "        X_full_train, y_full_train, test_size=len(y_full_train)-training_size+5, \n",
    "        stratify=y_full_train, random_state=42)\n",
    "    p_classifer = perceptron(iterations=500, learning_rate=0.2, seed=12)\n",
    "    p_classifer.fit(X_train, y_train)\n",
    "    y_pred = p_classifer.predict(X_test)\n",
    "    accuracy.append(np.sum((y_pred==y_test))/len(y_test))\n",
    "    print(f'training size: {X_train.shape[0]} \\t accuracy: {accuracy[-1]:5.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d6ac6bd-7465-4b19-9416-7e5d300596ee",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with '/bin/python3.11' requires the ipykernel package.\n",
      "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: '/bin/python3.11 -m pip install ipykernel -U --user --force-reinstall'"
     ]
    }
   ],
   "source": [
    "plt.figure(figsize=(6,4))\n",
    "plt.plot(training_sizes, accuracy, 'ob')\n",
    "plt.xlabel('training set size')\n",
    "plt.ylabel('accuracy')\n",
    "plt.ylim(0.5,1);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff0ba3ee-ac92-45f5-92e0-8d149ccbbdce",
   "metadata": {},
   "source": [
    "*Discussion of the plot here*.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b592783-24ac-4b98-a6fe-9273848f7a26",
   "metadata": {},
   "source": [
    "## Part 3 - Standardization\n",
    "\n",
    "Scaling your features is a core part of pre-processing data. One of the methods we have already seen in detail is that of [normalization](https://github.com/sarathsreedharan/CS345/blob/master/spring24/notebooks/module01_03_dot_products.ipynb). However, there are other methods. A popular one is called standardization. Under this method, you update each feature value such that it has zero mean and unit variation.\n",
    "\n",
    "You can find the details on how to do this at the following [wikipedia page](https://en.wikipedia.org/wiki/Feature_scaling#Standardization_(Z-score_Normalization)). Now, in this part, you have to implement the code to perform standardization (do not use the Sklearn function for it).\n",
    "\n",
    "1. Use that code on the Haberman's Dataset. Show how the mean of resultant feature values is zero and the standard deviation is one. Note that the method is applied across each individual feature. So, for a data matrix, the mean of the column is zero, and the standard deviation is one.\n",
    "2. Compare the accuracy obtained from Perceptron on Haberman's dataset, with and without standardization. As before, use tenfold cross-validation to compute accuracy and report the accuracy the same way you did for Part 1. Provide a small discussion on the results you see."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad4ca7de-46a7-4250-89c4-a766b8ff6ad4",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with '/bin/python3.11' requires the ipykernel package.\n",
      "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: '/bin/python3.11 -m pip install ipykernel -U --user --force-reinstall'"
     ]
    }
   ],
   "source": [
    "# Your code for standardization and it's application on the Haberman's dataset\n",
    "habber_man = \"haberman.csv\"\n",
    "\n",
    "hbLoad = pd.read_csv(habber_man)\n",
    "\n",
    "hbLoad_array = np.array(hbLoad)\n",
    "\n",
    "np.random.shuffle(hbLoad_array)\n",
    "\n",
    "print(np.shape(hbLoad_array))\n",
    "\n",
    "X = np.array(hbLoad_array[:, :-1])\n",
    "y = np.array(hbLoad_array[:, -1])\n",
    "y = np.where(y == 1, 1, -1)\n",
    "\n",
    "X.shape, y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe07b7a7-c7c8-4f20-be4f-b398460195a3",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with '/bin/python3.11' requires the ipykernel package.\n",
      "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: '/bin/python3.11 -m pip install ipykernel -U --user --force-reinstall'"
     ]
    }
   ],
   "source": [
    "X = (X - np.mean(X, axis=0))/np.std(X, axis=0)\n",
    "\n",
    "X = pd.DataFrame(X, columns = [\"Age\", \"Year\", \"Node\"])\n",
    "y = pd.DataFrame(y, columns = [\"Dead/Alive\"])\n",
    "\n",
    "hb_standardization = pd.concat([X, y], axis=1)\n",
    "\n",
    "hb_standardization.describe().round(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7578381f-0a03-4448-afa7-158186d37508",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with '/bin/python3.11' requires the ipykernel package.\n",
      "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: '/bin/python3.11 -m pip install ipykernel -U --user --force-reinstall'"
     ]
    }
   ],
   "source": [
    "hb_standardization = np.array(hb_standardization)\n",
    "\n",
    "np.random.shuffle(hb_standardization)\n",
    "\n",
    "print(np.shape(hb_standardization))\n",
    "\n",
    "\n",
    "X = np.array(hb_standardization[:, :-1])\n",
    "y = np.array(hb_standardization[:, -1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15bd48a0-f231-4907-a7f4-027b528677b9",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with '/bin/python3.11' requires the ipykernel package.\n",
      "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: '/bin/python3.11 -m pip install ipykernel -U --user --force-reinstall'"
     ]
    }
   ],
   "source": [
    "# Your code to evaluate perceptron on a standardized and non-standardized dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d136f0c-ae96-4944-937d-71ae69fa8fb9",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with '/bin/python3.11' requires the ipykernel package.\n",
      "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: '/bin/python3.11 -m pip install ipykernel -U --user --force-reinstall'"
     ]
    }
   ],
   "source": [
    "accuracy = []\n",
    "\n",
    "kf_classifer = KFold(n_splits=10, shuffle=True,random_state=5)\n",
    "\n",
    "for i, (train_index, test_index) in enumerate(kf.split(X)):\n",
    "    X_train, X_test = X[train_index], X[test_index]\n",
    "    y_train, y_test = y[train_index], y[test_index]\n",
    "\n",
    "    p_classifer = perceptron(iterations=500, learning_rate=0.02)\n",
    "    p_classifer.fit(X_train, y_train)\n",
    "    y_pred = p_classifer.predict(X_test)\n",
    "    acc = np.mean(y_test == y_pred)\n",
    "    accuracy.append(acc)\n",
    "\n",
    "p_hb_mean_standardization = np.mean(accuracy)\n",
    "\n",
    "print(\"10 fold for Habberman Dataset with Preceptron & Standardization\", p_hb_mean_standardization)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "378edd64-7c7d-4097-956f-f21987baf106",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with '/bin/python3.11' requires the ipykernel package.\n",
      "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: '/bin/python3.11 -m pip install ipykernel -U --user --force-reinstall'"
     ]
    }
   ],
   "source": [
    "kf_classifer = KFold(n_splits=10,shuffle=True,random_state=5)\n",
    "\n",
    "hb_average = []\n",
    "\n",
    "for i, (train, test) in enumerate(kf_classifer.split(X)):\n",
    "    X_train, X_test = X[train], X[test]\n",
    "    y_train, y_test = y[train], y[test]\n",
    "    \n",
    "    p = perceptron(iterations=250, learning_rate=0.01)\n",
    "    p.fit(X_train, y_train)\n",
    "    y_pred = p.predict(X_test)\n",
    "    av = np.mean(y_test == y_pred)\n",
    "    hb_average.append(av)\n",
    "\n",
    "ten_hab_average = np.mean(hb_average)\n",
    "\n",
    "print(\"10 fold for Habberman Dataset with Perceptron\", ten_hab_average)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11495b32-a0c0-42af-ba94-ca596286e62f",
   "metadata": {},
   "source": [
    "*Discussion of the results comparing perceptron on the two datset goes here*.\n",
    "\n",
    "Standarization was higher by 0.002-0.02 compared to the 10 fold with the perceptron, with both numbers side by side."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3932759-db47-452a-af65-a28403e2eb12",
   "metadata": {},
   "source": [
    "### Grading \n",
    "\n",
    "Although we will not grade on a 100 pt scale, the following is a sample grading sheet that will give you a basic weightage of the different questions:  \n",
    "\n",
    "```\n",
    "Grading sheet for assignment 1\n",
    "\n",
    "Part 1:  40 points.\n",
    "  Fixing labels: 10\n",
    "  Cross-validation code: 20\n",
    "  Comparison, reporting, and discussion: 20\n",
    "Part 2:  20 points.\n",
    "  Creation of the learning curve: 10 points\n",
    "  Discussion of the plot: 10 points\n",
    "Part 3:  40 points\n",
    "  Standardization code and demonstrating it on the dataset: 20 points\n",
    "  Comparison, reporting, and discussion: 20 points\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
