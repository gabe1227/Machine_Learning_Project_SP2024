{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "83b8a794-093e-4925-9488-c54e8b5915a7",
   "metadata": {},
   "source": [
    "# CS345 Spring 2024 Project - Gabriel Schonacher\n",
    "\n",
    "## March Madness Dataset - Predicition\n",
    "\n",
    "We are importing the March Madness Dataset and creating a prediction with the use of SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "6f98bf9c-9df7-4781-934e-b7fed0815aa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import cross_validate\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import cross_val_predict\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "0c9a4004-d6d6-408f-95dc-15e4580c91aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(362, 19)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "((362, 18), (362,))"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Eff_March_Madness = \"INT _ KenPom _ Efficiency.csv\"\n",
    "\n",
    "Eff_March_Madness = pd.read_csv(Eff_March_Madness)\n",
    "\n",
    "Eff_March_Madness_array = np.array(Eff_March_Madness)\n",
    "\n",
    "np.random.shuffle(Eff_March_Madness_array)\n",
    "\n",
    "print(np.shape(Eff_March_Madness_array))\n",
    "\n",
    "X = np.array(Eff_March_Madness_array[:, :-1])\n",
    "y = np.array(Eff_March_Madness_array[:, -1])\n",
    "\n",
    "X.shape, y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "75e7bace-f0fe-4347-9f5e-0e0e9d68b29a",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "could not convert string to float: 'North Dakota St.'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[44], line 6\u001b[0m\n\u001b[1;32m      1\u001b[0m X_train, X_test, y_train, y_test \u001b[38;5;241m=\u001b[39m train_test_split(\n\u001b[1;32m      2\u001b[0m     X, y, test_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.3\u001b[39m, random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m)\n\u001b[1;32m      4\u001b[0m svm_classifer \u001b[38;5;241m=\u001b[39m SVC(kernel\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlinear\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m----> 6\u001b[0m \u001b[43msvm_classifer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      7\u001b[0m y_pred \u001b[38;5;241m=\u001b[39m svm_classifer\u001b[38;5;241m.\u001b[39mpredict(X_test)\n\u001b[1;32m      9\u001b[0m p_hb_mean \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mmean(y_test \u001b[38;5;241m==\u001b[39m y_pred)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/base.py:1474\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[0;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1467\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[1;32m   1469\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[1;32m   1470\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[1;32m   1471\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[1;32m   1472\u001b[0m     )\n\u001b[1;32m   1473\u001b[0m ):\n\u001b[0;32m-> 1474\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/svm/_base.py:190\u001b[0m, in \u001b[0;36mBaseLibSVM.fit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m    188\u001b[0m     check_consistent_length(X, y)\n\u001b[1;32m    189\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 190\u001b[0m     X, y \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_validate_data\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    191\u001b[0m \u001b[43m        \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    192\u001b[0m \u001b[43m        \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    193\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfloat64\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    194\u001b[0m \u001b[43m        \u001b[49m\u001b[43morder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mC\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    195\u001b[0m \u001b[43m        \u001b[49m\u001b[43maccept_sparse\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcsr\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    196\u001b[0m \u001b[43m        \u001b[49m\u001b[43maccept_large_sparse\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    197\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    199\u001b[0m y \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_validate_targets(y)\n\u001b[1;32m    201\u001b[0m sample_weight \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39masarray(\n\u001b[1;32m    202\u001b[0m     [] \u001b[38;5;28;01mif\u001b[39;00m sample_weight \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m sample_weight, dtype\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39mfloat64\n\u001b[1;32m    203\u001b[0m )\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/base.py:650\u001b[0m, in \u001b[0;36mBaseEstimator._validate_data\u001b[0;34m(self, X, y, reset, validate_separately, cast_to_ndarray, **check_params)\u001b[0m\n\u001b[1;32m    648\u001b[0m         y \u001b[38;5;241m=\u001b[39m check_array(y, input_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124my\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mcheck_y_params)\n\u001b[1;32m    649\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 650\u001b[0m         X, y \u001b[38;5;241m=\u001b[39m \u001b[43mcheck_X_y\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mcheck_params\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    651\u001b[0m     out \u001b[38;5;241m=\u001b[39m X, y\n\u001b[1;32m    653\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m no_val_X \u001b[38;5;129;01mand\u001b[39;00m check_params\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mensure_2d\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mTrue\u001b[39;00m):\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/utils/validation.py:1263\u001b[0m, in \u001b[0;36mcheck_X_y\u001b[0;34m(X, y, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, multi_output, ensure_min_samples, ensure_min_features, y_numeric, estimator)\u001b[0m\n\u001b[1;32m   1258\u001b[0m         estimator_name \u001b[38;5;241m=\u001b[39m _check_estimator_name(estimator)\n\u001b[1;32m   1259\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   1260\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mestimator_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m requires y to be passed, but the target y is None\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1261\u001b[0m     )\n\u001b[0;32m-> 1263\u001b[0m X \u001b[38;5;241m=\u001b[39m \u001b[43mcheck_array\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1264\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1265\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccept_sparse\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maccept_sparse\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1266\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccept_large_sparse\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maccept_large_sparse\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1267\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1268\u001b[0m \u001b[43m    \u001b[49m\u001b[43morder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43morder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1269\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcopy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcopy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1270\u001b[0m \u001b[43m    \u001b[49m\u001b[43mforce_all_finite\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mforce_all_finite\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1271\u001b[0m \u001b[43m    \u001b[49m\u001b[43mensure_2d\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mensure_2d\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1272\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_nd\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mallow_nd\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1273\u001b[0m \u001b[43m    \u001b[49m\u001b[43mensure_min_samples\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mensure_min_samples\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1274\u001b[0m \u001b[43m    \u001b[49m\u001b[43mensure_min_features\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mensure_min_features\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1275\u001b[0m \u001b[43m    \u001b[49m\u001b[43mestimator\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1276\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mX\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1277\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1279\u001b[0m y \u001b[38;5;241m=\u001b[39m _check_y(y, multi_output\u001b[38;5;241m=\u001b[39mmulti_output, y_numeric\u001b[38;5;241m=\u001b[39my_numeric, estimator\u001b[38;5;241m=\u001b[39mestimator)\n\u001b[1;32m   1281\u001b[0m check_consistent_length(X, y)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/utils/validation.py:997\u001b[0m, in \u001b[0;36mcheck_array\u001b[0;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator, input_name)\u001b[0m\n\u001b[1;32m    995\u001b[0m         array \u001b[38;5;241m=\u001b[39m xp\u001b[38;5;241m.\u001b[39mastype(array, dtype, copy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m    996\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 997\u001b[0m         array \u001b[38;5;241m=\u001b[39m \u001b[43m_asarray_with_order\u001b[49m\u001b[43m(\u001b[49m\u001b[43marray\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43morder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43morder\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mxp\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mxp\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    998\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m ComplexWarning \u001b[38;5;28;01mas\u001b[39;00m complex_warning:\n\u001b[1;32m    999\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   1000\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mComplex data not supported\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(array)\n\u001b[1;32m   1001\u001b[0m     ) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mcomplex_warning\u001b[39;00m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/utils/_array_api.py:521\u001b[0m, in \u001b[0;36m_asarray_with_order\u001b[0;34m(array, dtype, order, copy, xp)\u001b[0m\n\u001b[1;32m    519\u001b[0m     array \u001b[38;5;241m=\u001b[39m numpy\u001b[38;5;241m.\u001b[39marray(array, order\u001b[38;5;241m=\u001b[39morder, dtype\u001b[38;5;241m=\u001b[39mdtype)\n\u001b[1;32m    520\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 521\u001b[0m     array \u001b[38;5;241m=\u001b[39m \u001b[43mnumpy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43masarray\u001b[49m\u001b[43m(\u001b[49m\u001b[43marray\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43morder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43morder\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    523\u001b[0m \u001b[38;5;66;03m# At this point array is a NumPy ndarray. We convert it to an array\u001b[39;00m\n\u001b[1;32m    524\u001b[0m \u001b[38;5;66;03m# container that is consistent with the input's namespace.\u001b[39;00m\n\u001b[1;32m    525\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m xp\u001b[38;5;241m.\u001b[39masarray(array)\n",
      "\u001b[0;31mValueError\u001b[0m: could not convert string to float: 'North Dakota St.'"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.3, random_state=5)\n",
    "\n",
    "svm_classifer = SVC(kernel=\"linear\")\n",
    "\n",
    "svm_classifer.fit(X_train, y_train)\n",
    "y_pred = svm_classifer.predict(X_test)\n",
    "\n",
    "p_hb_mean = np.mean(y_test == y_pred)\n",
    "p_hb_std = np.std(y_test == y_pred)\n",
    "\n",
    "print(\"SVM Habber-man Average: \", p_hb_mean)\n",
    "print(\"SVM Habber-man Standard Deviation: \", p_hb_std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "e37efab3-405f-465d-b5a4-9e712b8e5888",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'perceptron' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[45], line 9\u001b[0m\n\u001b[1;32m      6\u001b[0m X_train, X_test \u001b[38;5;241m=\u001b[39m X[train], X[test]\n\u001b[1;32m      7\u001b[0m y_train, y_test \u001b[38;5;241m=\u001b[39m y[train], y[test]\n\u001b[0;32m----> 9\u001b[0m p \u001b[38;5;241m=\u001b[39m \u001b[43mperceptron\u001b[49m(iterations\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m250\u001b[39m, learning_rate\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.01\u001b[39m)\n\u001b[1;32m     10\u001b[0m p\u001b[38;5;241m.\u001b[39mfit(X_train, y_train)\n\u001b[1;32m     11\u001b[0m y_pred \u001b[38;5;241m=\u001b[39m p\u001b[38;5;241m.\u001b[39mpredict(X_test)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'perceptron' is not defined"
     ]
    }
   ],
   "source": [
    "kf_classifer = KFold(n_splits=10,shuffle=True,random_state=5)\n",
    "\n",
    "hb_average = []\n",
    "\n",
    "for i, (train, test) in enumerate(kf_classifer.split(X)):\n",
    "    X_train, X_test = X[train], X[test]\n",
    "    y_train, y_test = y[train], y[test]\n",
    "    \n",
    "    p = perceptron(iterations=250, learning_rate=0.01)\n",
    "    p.fit(X_train, y_train)\n",
    "    y_pred = p.predict(X_test)\n",
    "    av = np.mean(y_test == y_pred)\n",
    "    hb_average.append(av)\n",
    "\n",
    "ten_hab_average = np.mean(hb_average)\n",
    "\n",
    "print(\"10 fold for Habberman Dataset with Perceptron\", ten_hab_average)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66f4399f-51b3-4d03-a8f5-2d44c9f68205",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with '/bin/python3.11' requires the ipykernel package.\n",
      "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: '/bin/python3.11 -m pip install ipykernel -U --user --force-reinstall'"
     ]
    }
   ],
   "source": [
    "kf_classifer = KFold(n_splits=10,shuffle=True,random_state=5)\n",
    "\n",
    "hb_average = []\n",
    "\n",
    "for i, (train, test) in enumerate(kf_classifer.split(X)):\n",
    "    X_train, X_test = X[train], X[test]\n",
    "    y_train, y_test = y[train], y[test]\n",
    "    \n",
    "    svm_classifer = SVC(kernel=\"linear\")\n",
    "    svm_classifer.fit(X_train, y_train)\n",
    "    y_pred = svm_classifer.predict(X_test)\n",
    "    av = np.mean(y_test == y_pred)\n",
    "    hb_average.append(av)\n",
    "\n",
    "ten_hab_average = np.mean(hb_average)\n",
    "\n",
    "print(\"10 fold for Habberman Dataset with SVC\", ten_hab_average)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0327f2ae-a6f7-4dd0-a1b8-d634b519db06",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with '/bin/python3.11' requires the ipykernel package.\n",
      "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: '/bin/python3.11 -m pip install ipykernel -U --user --force-reinstall'"
     ]
    }
   ],
   "source": [
    "data = load_breast_cancer()\n",
    "\n",
    "X = data.data\n",
    "y = data.target\n",
    "\n",
    "y = np.where(y == 1, 1, -1)\n",
    "\n",
    "X.shape, y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fdb8f7d-ff50-4ffe-880c-0402e14653ea",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with '/bin/python3.11' requires the ipykernel package.\n",
      "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: '/bin/python3.11 -m pip install ipykernel -U --user --force-reinstall'"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.4, \n",
    "                                                    random_state=0)\n",
    "\n",
    "svm_classifer = SVC(kernel=\"linear\")\n",
    "\n",
    "svm_classifer.fit(X_train, y_train)\n",
    "y_pred = svm_classifer.predict(X_test)\n",
    "\n",
    "breast_cancer_average = np.mean(y_pred == y_test)\n",
    "breast_cancer_standard_deviation = np.std(y_pred == y_test)\n",
    "\n",
    "print(\"SVM Breast Cancer Average: \", breast_cancer_average)\n",
    "print(\"SVM Breast Cacner Standard Deviation: \", breast_cancer_standard_deviation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78b331b4-a44b-498b-88ab-330f3c4fc385",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with '/bin/python3.11' requires the ipykernel package.\n",
      "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: '/bin/python3.11 -m pip install ipykernel -U --user --force-reinstall'"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.4, \n",
    "                                                    random_state=0)\n",
    "\n",
    "p = perceptron(iterations = 250, learning_rate = 0.1)\n",
    "p.fit(X_train, y_train)\n",
    "y_pred = p.predict(X_test)\n",
    "\n",
    "breast_cancer_average = np.mean(y_pred == y_test)\n",
    "breast_cancer_standard_deviation = np.std(y_pred == y_test)\n",
    "\n",
    "print(\"Perceptron Breast Cancer Average: \", breast_cancer_average)\n",
    "print(\"Perceptron Breast Cacner Standard Deviation: \", breast_cancer_standard_deviation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e72f0ec-2641-4280-96a0-d5938b07b8b4",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with '/bin/python3.11' requires the ipykernel package.\n",
      "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: '/bin/python3.11 -m pip install ipykernel -U --user --force-reinstall'"
     ]
    }
   ],
   "source": [
    "kf_classifer = KFold(n_splits=10,shuffle=True,random_state=5)\n",
    "\n",
    "breCan_average = []\n",
    "\n",
    "for i, (train, test) in enumerate(kf_classifer.split(X)):\n",
    "    X_train, X_test = X[train], X[test]\n",
    "    y_train, y_test = y[train], y[test]\n",
    "    \n",
    "    svm_classifer = SVC(kernel=\"linear\")\n",
    "    svm_classifer.fit(X_train, y_train)\n",
    "    y_pred = svm_classifer.predict(X_test)\n",
    "    av = np.mean(y_test == y_pred)\n",
    "    breCan_average.append(av)\n",
    "\n",
    "ten_breCan_average = np.mean(breCan_average)\n",
    "\n",
    "print(\"10 fold for Breast Cancer Dataset with SVC\", ten_breCan_average)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c07d9601-fde4-444e-b446-10c209173aa2",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with '/bin/python3.11' requires the ipykernel package.\n",
      "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: '/bin/python3.11 -m pip install ipykernel -U --user --force-reinstall'"
     ]
    }
   ],
   "source": [
    "kf_classifer = KFold(n_splits=10,shuffle=True,random_state=5)\n",
    "\n",
    "breCan_average = []\n",
    "\n",
    "for i, (train, test) in enumerate(kf_classifer.split(X)):\n",
    "    X_train, X_test = X[train], X[test]\n",
    "    y_train, y_test = y[train], y[test]\n",
    "    \n",
    "    p_classifer = perceptron(iterations=1000, learning_rate=0.02, seed=5)\n",
    "    p_classifer.fit(X_train, y_train)\n",
    "    y_pred = p_classifer.predict(X_test)\n",
    "    av = np.mean(y_test == y_pred)\n",
    "    breCan_average.append(av)\n",
    "\n",
    "ten_breCan_average = np.mean(breCan_average)\n",
    "\n",
    "print(\"10 fold for Breast Cancer Dataset with Preceptron\", ten_breCan_average)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6a4dcb4-ce3d-49a2-ae16-916da669f39f",
   "metadata": {},
   "source": [
    "*Discussion of the results here*.\n",
    "\n",
    "\n",
    "*The Habber-Man Average is lower than the Breast Cancer data set, and the Standard Deviation is higher in the higher in the Habber-man dataset than the Breast Cancer dataset making the Breast Cancer dataset having more consistency. While the Standard Deviation in the Habber-man data is higher, suggesting that it has higher variabilty than the Breast Cancer dataset. While the 10 Fold for Breast cancer test for the preceptron and the SVC is relativley higher than the perceptron. While for the Habberman data set in regards to the 10 Fold, it is higher of an accuracy for the SVC compared to the perceptron*."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c3ce3b7-61bd-45f9-a775-e89fd8e69cd4",
   "metadata": {},
   "source": [
    "## Part 2 - Learning Curve\n",
    "\n",
    "We looked briefly at the idea of learning curves in the notebook for cross-validation. It looked at how accuracy changed with respect to the number of training examples. In this part of the assignment, plot a learning curve for the perceptron algorithm for an increasing number of training examples. To plot this, \n",
    "1. First, create a held-out validation set against which you will be comparing all your trained models\n",
    "2. Now, from the remaining data set, create train sets of increasing sizes. To create more compact plots you can use a logarithmic scale like 10, 20, 40, 80, 160, etc..\n",
    "\n",
    "For the plot X-axis should be the training data size considered and the Y-axis should be the accuracy of the model (obtained by training on the dataset of that size) as measured on the held-out validation set.  \n",
    "\n",
    "After receiving the plot, make sure to discuss your observations about the plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef3256b0-c4b2-4b43-bea6-a2454d86ba95",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with '/bin/python3.11' requires the ipykernel package.\n",
      "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: '/bin/python3.11 -m pip install ipykernel -U --user --force-reinstall'"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_breast_cancer\n",
    "\n",
    "data = load_breast_cancer()\n",
    "\n",
    "X = data.data\n",
    "y = data.target\n",
    "\n",
    "y = np.where(y == 1, 1, -1)\n",
    "\n",
    "X.shape, y.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c246835e-57dd-4cb3-ba38-5c2558ac6dd8",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with '/bin/python3.11' requires the ipykernel package.\n",
      "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: '/bin/python3.11 -m pip install ipykernel -U --user --force-reinstall'"
     ]
    }
   ],
   "source": [
    "X_full_train, X_test, y_full_train, y_test = train_test_split(\n",
    "    X, y, test_size = len(y)-max(training_sizes), \n",
    "    stratify=y, random_state=1)\n",
    "\n",
    "print(f'We have selected {X_test.shape[0]} test images from {X.shape[0]} images total.')\n",
    "print(f'The remaining {X_full_train.shape[0]} images are available for training.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29f4b71d-f062-4407-839d-765191f9c2b1",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with '/bin/python3.11' requires the ipykernel package.\n",
      "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: '/bin/python3.11 -m pip install ipykernel -U --user --force-reinstall'"
     ]
    }
   ],
   "source": [
    "training_sizes = [10, 20, 40, 80, 160]\n",
    "\n",
    "accuracy = []\n",
    "\n",
    "for training_size in training_sizes:\n",
    "    X_train,_ , y_train,_ = train_test_split(\n",
    "        X_full_train, y_full_train, test_size=len(y_full_train)-training_size+5, \n",
    "        stratify=y_full_train, random_state=42)\n",
    "    p_classifer = perceptron(iterations=500, learning_rate=0.2, seed=12)\n",
    "    p_classifer.fit(X_train, y_train)\n",
    "    y_pred = p_classifer.predict(X_test)\n",
    "    accuracy.append(np.sum((y_pred==y_test))/len(y_test))\n",
    "    print(f'training size: {X_train.shape[0]} \\t accuracy: {accuracy[-1]:5.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d6ac6bd-7465-4b19-9416-7e5d300596ee",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with '/bin/python3.11' requires the ipykernel package.\n",
      "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: '/bin/python3.11 -m pip install ipykernel -U --user --force-reinstall'"
     ]
    }
   ],
   "source": [
    "plt.figure(figsize=(6,4))\n",
    "plt.plot(training_sizes, accuracy, 'ob')\n",
    "plt.xlabel('training set size')\n",
    "plt.ylabel('accuracy')\n",
    "plt.ylim(0.5,1);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff0ba3ee-ac92-45f5-92e0-8d149ccbbdce",
   "metadata": {},
   "source": [
    "*Discussion of the plot here*.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b592783-24ac-4b98-a6fe-9273848f7a26",
   "metadata": {},
   "source": [
    "## Part 3 - Standardization\n",
    "\n",
    "Scaling your features is a core part of pre-processing data. One of the methods we have already seen in detail is that of [normalization](https://github.com/sarathsreedharan/CS345/blob/master/spring24/notebooks/module01_03_dot_products.ipynb). However, there are other methods. A popular one is called standardization. Under this method, you update each feature value such that it has zero mean and unit variation.\n",
    "\n",
    "You can find the details on how to do this at the following [wikipedia page](https://en.wikipedia.org/wiki/Feature_scaling#Standardization_(Z-score_Normalization)). Now, in this part, you have to implement the code to perform standardization (do not use the Sklearn function for it).\n",
    "\n",
    "1. Use that code on the Haberman's Dataset. Show how the mean of resultant feature values is zero and the standard deviation is one. Note that the method is applied across each individual feature. So, for a data matrix, the mean of the column is zero, and the standard deviation is one.\n",
    "2. Compare the accuracy obtained from Perceptron on Haberman's dataset, with and without standardization. As before, use tenfold cross-validation to compute accuracy and report the accuracy the same way you did for Part 1. Provide a small discussion on the results you see."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad4ca7de-46a7-4250-89c4-a766b8ff6ad4",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with '/bin/python3.11' requires the ipykernel package.\n",
      "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: '/bin/python3.11 -m pip install ipykernel -U --user --force-reinstall'"
     ]
    }
   ],
   "source": [
    "# Your code for standardization and it's application on the Haberman's dataset\n",
    "habber_man = \"haberman.csv\"\n",
    "\n",
    "hbLoad = pd.read_csv(habber_man)\n",
    "\n",
    "hbLoad_array = np.array(hbLoad)\n",
    "\n",
    "np.random.shuffle(hbLoad_array)\n",
    "\n",
    "print(np.shape(hbLoad_array))\n",
    "\n",
    "X = np.array(hbLoad_array[:, :-1])\n",
    "y = np.array(hbLoad_array[:, -1])\n",
    "y = np.where(y == 1, 1, -1)\n",
    "\n",
    "X.shape, y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe07b7a7-c7c8-4f20-be4f-b398460195a3",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with '/bin/python3.11' requires the ipykernel package.\n",
      "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: '/bin/python3.11 -m pip install ipykernel -U --user --force-reinstall'"
     ]
    }
   ],
   "source": [
    "X = (X - np.mean(X, axis=0))/np.std(X, axis=0)\n",
    "\n",
    "X = pd.DataFrame(X, columns = [\"Age\", \"Year\", \"Node\"])\n",
    "y = pd.DataFrame(y, columns = [\"Dead/Alive\"])\n",
    "\n",
    "hb_standardization = pd.concat([X, y], axis=1)\n",
    "\n",
    "hb_standardization.describe().round(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7578381f-0a03-4448-afa7-158186d37508",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with '/bin/python3.11' requires the ipykernel package.\n",
      "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: '/bin/python3.11 -m pip install ipykernel -U --user --force-reinstall'"
     ]
    }
   ],
   "source": [
    "hb_standardization = np.array(hb_standardization)\n",
    "\n",
    "np.random.shuffle(hb_standardization)\n",
    "\n",
    "print(np.shape(hb_standardization))\n",
    "\n",
    "\n",
    "X = np.array(hb_standardization[:, :-1])\n",
    "y = np.array(hb_standardization[:, -1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15bd48a0-f231-4907-a7f4-027b528677b9",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with '/bin/python3.11' requires the ipykernel package.\n",
      "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: '/bin/python3.11 -m pip install ipykernel -U --user --force-reinstall'"
     ]
    }
   ],
   "source": [
    "# Your code to evaluate perceptron on a standardized and non-standardized dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d136f0c-ae96-4944-937d-71ae69fa8fb9",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with '/bin/python3.11' requires the ipykernel package.\n",
      "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: '/bin/python3.11 -m pip install ipykernel -U --user --force-reinstall'"
     ]
    }
   ],
   "source": [
    "accuracy = []\n",
    "\n",
    "kf_classifer = KFold(n_splits=10, shuffle=True,random_state=5)\n",
    "\n",
    "for i, (train_index, test_index) in enumerate(kf.split(X)):\n",
    "    X_train, X_test = X[train_index], X[test_index]\n",
    "    y_train, y_test = y[train_index], y[test_index]\n",
    "\n",
    "    p_classifer = perceptron(iterations=500, learning_rate=0.02)\n",
    "    p_classifer.fit(X_train, y_train)\n",
    "    y_pred = p_classifer.predict(X_test)\n",
    "    acc = np.mean(y_test == y_pred)\n",
    "    accuracy.append(acc)\n",
    "\n",
    "p_hb_mean_standardization = np.mean(accuracy)\n",
    "\n",
    "print(\"10 fold for Habberman Dataset with Preceptron & Standardization\", p_hb_mean_standardization)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "378edd64-7c7d-4097-956f-f21987baf106",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with '/bin/python3.11' requires the ipykernel package.\n",
      "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: '/bin/python3.11 -m pip install ipykernel -U --user --force-reinstall'"
     ]
    }
   ],
   "source": [
    "kf_classifer = KFold(n_splits=10,shuffle=True,random_state=5)\n",
    "\n",
    "hb_average = []\n",
    "\n",
    "for i, (train, test) in enumerate(kf_classifer.split(X)):\n",
    "    X_train, X_test = X[train], X[test]\n",
    "    y_train, y_test = y[train], y[test]\n",
    "    \n",
    "    p = perceptron(iterations=250, learning_rate=0.01)\n",
    "    p.fit(X_train, y_train)\n",
    "    y_pred = p.predict(X_test)\n",
    "    av = np.mean(y_test == y_pred)\n",
    "    hb_average.append(av)\n",
    "\n",
    "ten_hab_average = np.mean(hb_average)\n",
    "\n",
    "print(\"10 fold for Habberman Dataset with Perceptron\", ten_hab_average)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11495b32-a0c0-42af-ba94-ca596286e62f",
   "metadata": {},
   "source": [
    "*Discussion of the results comparing perceptron on the two datset goes here*.\n",
    "\n",
    "Standarization was higher by 0.002-0.02 compared to the 10 fold with the perceptron, with both numbers side by side."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3932759-db47-452a-af65-a28403e2eb12",
   "metadata": {},
   "source": [
    "### Grading \n",
    "\n",
    "Although we will not grade on a 100 pt scale, the following is a sample grading sheet that will give you a basic weightage of the different questions:  \n",
    "\n",
    "```\n",
    "Grading sheet for assignment 1\n",
    "\n",
    "Part 1:  40 points.\n",
    "  Fixing labels: 10\n",
    "  Cross-validation code: 20\n",
    "  Comparison, reporting, and discussion: 20\n",
    "Part 2:  20 points.\n",
    "  Creation of the learning curve: 10 points\n",
    "  Discussion of the plot: 10 points\n",
    "Part 3:  40 points\n",
    "  Standardization code and demonstrating it on the dataset: 20 points\n",
    "  Comparison, reporting, and discussion: 20 points\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
